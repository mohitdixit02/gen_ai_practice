{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec8083ee",
   "metadata": {},
   "source": [
    "#### Text Splitter Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0090f5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter, Language\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5de743e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': '../DocLoadersPractice/files/sample.txt'}\n",
      "Hi this is sample text for LangChain DocLoader practice.\n",
      "It contains multiple lines.\n",
      "The HuggingFacePipeline in LangChain is a wrapper class that allows you to use models from the Hugging Face transformers library, run them locally, and integrate them into LangChain applications. \n",
      "Overview\n",
      "Purpose: The class wraps the Hugging Face pipeline function, enabling seamless use of a wide range of open-source models (like GPT-2 or T5) within the LangChain framework.\n",
      "Local Execution: It runs models locally on your computer, making it suitable for privacy-sensitive applications or environments where an internet connection is unavailable after the initial download.\n",
      "Supported Tasks: It primarily supports text-centric tasks such as text generation, text-to-text generation, and summarization. \n"
     ]
    }
   ],
   "source": [
    "# Sample Txt from DocLoaders Practie\n",
    "file_path = \"../DocLoadersPractice/files/sample.txt\"\n",
    "text_loader = TextLoader(file_path)\n",
    "documents = text_loader.load()\n",
    "for doc in documents:\n",
    "    print(doc.metadata)\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e075ed52",
   "metadata": {},
   "source": [
    "Length Based Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6027092f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 196, which is longer than the specified 100\n",
      "Created a chunk of size 171, which is longer than the specified 100\n",
      "Created a chunk of size 199, which is longer than the specified 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Document 1 ---\n",
      "Hi this is sample text for LangChain DocLoader practice.\n",
      "It contains multiple lines.\n",
      "\n",
      "--- Document 2 ---\n",
      "The HuggingFacePipeline in LangChain is a wrapper class that allows you to use models from the Hugging Face transformers library, run them locally, and integrate them into LangChain applications.\n",
      "\n",
      "--- Document 3 ---\n",
      "Overview\n",
      "\n",
      "--- Document 4 ---\n",
      "Purpose: The class wraps the Hugging Face pipeline function, enabling seamless use of a wide range of open-source models (like GPT-2 or T5) within the LangChain framework.\n",
      "\n",
      "--- Document 5 ---\n",
      "Local Execution: It runs models locally on your computer, making it suitable for privacy-sensitive applications or environments where an internet connection is unavailable after the initial download.\n",
      "\n",
      "--- Document 6 ---\n",
      "Supported Tasks: It primarily supports text-centric tasks such as text generation, text-to-text generation, and summarization.\n"
     ]
    }
   ],
   "source": [
    "lb_splitter = CharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=0,\n",
    "    separator=\"\\n\",\n",
    ")\n",
    "lb_result = lb_splitter.split_documents(documents)\n",
    "for i, doc in enumerate(lb_result):\n",
    "    print(f\"\\n--- Document {i+1} ---\")\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea4013cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Document 1 ---\n",
      "Hi this is sample text for LangChain DocLoader practice.\n",
      "It contains multiple lines.\n",
      "The HuggingFace\n",
      "\n",
      "--- Document 2 ---\n",
      "Pipeline in LangChain is a wrapper class that allows you to use models from the Hugging Face transfo\n",
      "\n",
      "--- Document 3 ---\n",
      "rmers library, run them locally, and integrate them into LangChain applications. \n",
      "Overview\n",
      "Purpose:\n",
      "\n",
      "--- Document 4 ---\n",
      "The class wraps the Hugging Face pipeline function, enabling seamless use of a wide range of open-so\n",
      "\n",
      "--- Document 5 ---\n",
      "urce models (like GPT-2 or T5) within the LangChain framework.\n",
      "Local Execution: It runs models local\n",
      "\n",
      "--- Document 6 ---\n",
      "ly on your computer, making it suitable for privacy-sensitive applications or environments where an\n",
      "\n",
      "--- Document 7 ---\n",
      "internet connection is unavailable after the initial download.\n",
      "Supported Tasks: It primarily support\n",
      "\n",
      "--- Document 8 ---\n",
      "s text-centric tasks such as text generation, text-to-text generation, and summarization.\n"
     ]
    }
   ],
   "source": [
    "lb_splitter = CharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=0,\n",
    "    separator=\"\",\n",
    ")\n",
    "lb_result = lb_splitter.split_documents(documents)\n",
    "for i, doc in enumerate(lb_result):\n",
    "    print(f\"\\n--- Document {i+1} ---\")\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9067b4",
   "metadata": {},
   "source": [
    "Text Structure Based Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0f7e9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Document 1 ---\n",
      "Hi this is sample text for LangChain DocLoader practice.\n",
      "It contains multiple lines.\n",
      "\n",
      "--- Document 2 ---\n",
      "The HuggingFacePipeline in LangChain is a wrapper class that allows you to use models from the Hugging Face transformers library, run them locally, and integrate them into LangChain applications.\n",
      "\n",
      "--- Document 3 ---\n",
      "Overview\n",
      "Purpose: The class wraps the Hugging Face pipeline function, enabling seamless use of a wide range of open-source models (like GPT-2 or T5) within the LangChain framework.\n",
      "\n",
      "--- Document 4 ---\n",
      "Local Execution: It runs models locally on your computer, making it suitable for privacy-sensitive applications or environments where an internet connection is unavailable after the initial download.\n",
      "\n",
      "--- Document 5 ---\n",
      "Supported Tasks: It primarily supports text-centric tasks such as text generation, text-to-text generation, and summarization.\n"
     ]
    }
   ],
   "source": [
    "ts_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "\n",
    "ts_result = ts_splitter.split_documents(documents)\n",
    "for i, doc in enumerate(ts_result):\n",
    "    print(f\"\\n--- Document {i+1} ---\")\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccf206c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Document 1 ---\n",
      "Hi this is sample text for LangChain DocLoader practice.\n",
      "It contains multiple lines.\n",
      "\n",
      "--- Document 2 ---\n",
      "The HuggingFacePipeline in LangChain is a wrapper class that allows you to use models from the\n",
      "\n",
      "--- Document 3 ---\n",
      "Hugging Face transformers library, run them locally, and integrate them into LangChain\n",
      "\n",
      "--- Document 4 ---\n",
      "applications.\n",
      "\n",
      "--- Document 5 ---\n",
      "Overview\n",
      "\n",
      "--- Document 6 ---\n",
      "Purpose: The class wraps the Hugging Face pipeline function, enabling seamless use of a wide range\n",
      "\n",
      "--- Document 7 ---\n",
      "of open-source models (like GPT-2 or T5) within the LangChain framework.\n",
      "\n",
      "--- Document 8 ---\n",
      "Local Execution: It runs models locally on your computer, making it suitable for privacy-sensitive\n",
      "\n",
      "--- Document 9 ---\n",
      "applications or environments where an internet connection is unavailable after the initial\n",
      "\n",
      "--- Document 10 ---\n",
      "download.\n",
      "\n",
      "--- Document 11 ---\n",
      "Supported Tasks: It primarily supports text-centric tasks such as text generation, text-to-text\n",
      "\n",
      "--- Document 12 ---\n",
      "generation, and summarization.\n"
     ]
    }
   ],
   "source": [
    "ts_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "\n",
    "ts_result = ts_splitter.split_documents(documents)\n",
    "for i, doc in enumerate(ts_result):\n",
    "    print(f\"\\n--- Document {i+1} ---\")\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a3c7a7",
   "metadata": {},
   "source": [
    "Markdown Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d12d4592",
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown_Sample = \"\"\"\n",
    "### Sample Markdown Document\n",
    "This is a sample markdown document to demonstrate the MarkdownHeaderTextSplitter.\n",
    "# Header 1\n",
    "This is some content under header 1.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1565dfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chunk 1 ---\n",
      "### Sample Markdown Document\n",
      "\n",
      "--- Chunk 2 ---\n",
      "This is a sample markdown document to demonstrate the MarkdownHeaderTextSplitter.\n",
      "\n",
      "--- Chunk 3 ---\n",
      "# Header 1\n",
      "This is some content under header 1.\n"
     ]
    }
   ],
   "source": [
    "md_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.MARKDOWN,\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "md_result = md_splitter.split_text(Markdown_Sample)\n",
    "for i, chunk in enumerate(md_result):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\")\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84197da3",
   "metadata": {},
   "source": [
    "Python Code Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b613dafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Code Chunk 1 ---\n",
      "def greet(name):\n",
      "    print(f\"Hello, {name}!\")\n",
      "\n",
      "--- Code Chunk 2 ---\n",
      "greet(\"World\")\n"
     ]
    }
   ],
   "source": [
    "sample_code = \"\"\"\n",
    "def greet(name):\n",
    "    print(f\"Hello, {name}!\")\n",
    "greet(\"World\")\n",
    "\"\"\"\n",
    "code_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON,\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "code_result = code_splitter.split_text(sample_code)\n",
    "for i, chunk in enumerate(code_result):\n",
    "    print(f\"\\n--- Code Chunk {i+1} ---\")\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1b83ad",
   "metadata": {},
   "source": [
    "Semantic Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605d34be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c311f06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Softwares\\DL\\conda\\envs\\lc_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in D:\\Development\\ML\\Deep Learning\\GenAI\\.hf_cache\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cache_dir = 'D:/Development/ML/Deep Learning/GenAI/.hf_cache'\n",
    "os.environ['HF_HOME'] = cache_dir\n",
    "os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
    "os.environ['HF_DATASETS_CACHE'] = cache_dir\n",
    "os.environ[\"SENTENCE_TRANSFORMERS_HOME\"] = cache_dir\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "SentenceModel = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "17f01b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Semantic Chunk 1 ---\n",
      "Hi this is sample text for LangChain DocLoader practice. It contains multiple lines. The HuggingFacePipeline in LangChain is a wrapper class that allows you to use models from the Hugging Face transformers library, run them locally, and integrate them into LangChain applications. Overview\n",
      "Purpose: The class wraps the Hugging Face pipeline function, enabling seamless use of a wide range of open-source models (like GPT-2 or T5) within the LangChain framework. Local Execution: It runs models locally on your computer, making it suitable for privacy-sensitive applications or environments where an internet connection is unavailable after the initial download.\n",
      "\n",
      "--- Semantic Chunk 2 ---\n",
      "Supported Tasks: It primarily supports text-centric tasks such as text generation, text-to-text generation, and summarization. \n"
     ]
    }
   ],
   "source": [
    "smn_chunker = SemanticChunker(\n",
    "    embeddings=SentenceModel,\n",
    "    breakpoint_threshold_type='standard_deviation',\n",
    "    breakpoint_threshold_amount=1\n",
    ")\n",
    "docs = [doc.page_content for doc in documents]\n",
    "smn_res = smn_chunker.create_documents(docs)\n",
    "for i, doc in enumerate(smn_res):\n",
    "    print(f\"\\n--- Semantic Chunk {i+1} ---\")\n",
    "    print(doc.page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
